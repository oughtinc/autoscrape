{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Automatic scraping of research publications for Elicit search tasks",
   "metadata": {
    "tags": [],
    "cell_id": "00000-a8aa8d8e-64e4-48d3-bb2d-27831854a60f",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "This notebook contains code to automate the retrieval of lists of research papers, typically from the websites of third-party think tanks.\n\nBetween websites, this task differs mainly in the classes or paths of the HTML elements that contain each research paper's title, authors, date, etc. Therefore, this notebook contains one function that is (hopefully) powerful and generic enough to scrape any think tank website, given a small Python dict that specifies how to do so for the website in question.\nA number of such specificiation dicts are then provided for think tanks covering the development and societal effects of technology.\n\nThe goal of this effort is to assist users in creating such specifications for new websites. Evolving attempts can be found at the bottom of the notebook.",
   "metadata": {
    "tags": [],
    "cell_id": "00002-3f2ece3f-e3b8-4c45-9fe3-82f8ae44e90c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "First, we install any non-standard Python libraries:",
   "metadata": {
    "tags": [],
    "cell_id": "00002-36ccb211-a30f-4118-9f14-a350741490c5",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-dc156a08-9ac0-4af9-853a-21d5017de46f",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2a93d087",
    "execution_start": 1627321349103,
    "execution_millis": 23214,
    "is_code_hidden": false,
    "is_output_hidden": false,
    "deepnote_cell_type": "code"
   },
   "source": "!pip install jsonpath_ng\n!pip install pyppeteer\n!pyppeteer-install\n!pip install nest_asyncio\n!apt install chromium  # pyppeteer downloads its own chromium, but gotta pull in the gazillion libraries",
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting jsonpath_ng\n  Downloading jsonpath_ng-1.5.3-py3-none-any.whl (29 kB)\nRequirement already satisfied: decorator in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from jsonpath_ng) (5.0.9)\nRequirement already satisfied: six in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from jsonpath_ng) (1.16.0)\nCollecting ply\n  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n\u001b[K     |████████████████████████████████| 49 kB 4.5 MB/s \n\u001b[?25hInstalling collected packages: ply, jsonpath-ng\nSuccessfully installed jsonpath-ng-1.5.3 ply-3.11\n\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nCollecting pyppeteer\n  Downloading pyppeteer-0.2.5-py3-none-any.whl (87 kB)\n\u001b[K     |████████████████████████████████| 87 kB 11.0 MB/s \n\u001b[?25hCollecting pyee<9.0.0,>=8.1.0\n  Downloading pyee-8.1.0-py2.py3-none-any.whl (12 kB)\nRequirement already satisfied: tqdm<5.0.0,>=4.42.1 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from pyppeteer) (4.61.2)\nCollecting importlib-metadata<3.0.0,>=2.1.1\n  Downloading importlib_metadata-2.1.1-py2.py3-none-any.whl (10 kB)\nRequirement already satisfied: urllib3<2.0.0,>=1.25.8 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from pyppeteer) (1.26.6)\nCollecting websockets<9.0,>=8.1\n  Downloading websockets-8.1-cp37-cp37m-manylinux2010_x86_64.whl (79 kB)\n\u001b[K     |████████████████████████████████| 79 kB 16.8 MB/s \n\u001b[?25hCollecting appdirs<2.0.0,>=1.4.3\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\nRequirement already satisfied: zipp>=0.5 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from importlib-metadata<3.0.0,>=2.1.1->pyppeteer) (3.5.0)\nInstalling collected packages: websockets, pyee, importlib-metadata, appdirs, pyppeteer\n  Attempting uninstall: importlib-metadata\n    Found existing installation: importlib-metadata 3.10.1\n    Not uninstalling importlib-metadata at /shared-libs/python3.7/py-core/lib/python3.7/site-packages, outside environment /root/venv\n    Can't uninstall 'importlib-metadata'. No files were found to uninstall.\nSuccessfully installed appdirs-1.4.4 importlib-metadata-2.1.1 pyee-8.1.0 pyppeteer-0.2.5 websockets-8.1\n\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n[W:pyppeteer.chromium_downloader] start chromium download.\nDownload may take a few minutes.\n100%|██████████████████████| 108773488/108773488 [00:00<00:00, 188694002.18it/s]\n[W:pyppeteer.chromium_downloader] \nchromium download done.\n[W:pyppeteer.chromium_downloader] chromium extracted to: /root/.local/share/pyppeteer/local-chromium/588429\nRequirement already satisfied: nest_asyncio in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (1.5.1)\n\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n\n\n\nE: Unable to locate package chromium\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "The following `Extractor` class implements the generic scraping procedure:",
   "metadata": {
    "tags": [],
    "cell_id": "00004-0fb49da7-8521-4c10-a647-c90454a79eaa",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00005-45d6c0ee-49d6-4aa0-9660-41a27fc37ae6",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1e08b1b3",
    "execution_start": 1627321372355,
    "execution_millis": 184,
    "deepnote_cell_type": "code"
   },
   "source": "import requests\nfrom io import StringIO\nfrom datetime import datetime\nimport time\nfrom lxml import etree\nfrom lxml.etree import XML, ElementTree\nimport lxml.html\nimport sys\n\ntry:\n    import pyppeteer\n    import asyncio\n    HAS_PYPPETEER = True\nexcept Exception as e:\n    print(\"Could not import Pyppeteer\", e)\n    HAS_PYPPETEER = False\n\nimport jsonpath_ng\nimport json\nimport traceback\n\n\n\n\nimport nest_asyncio\nnest_asyncio.apply()\n\n\n\nSEPARATOR = \"\\n\\n\\n\"\n\nimport logging\nlog = logging.getLogger(__name__)\n#log_handler = logging.StreamHandler(sys.stdout)\n#log.addHandler(log_handler)\nlogging.getLogger(__name__).setLevel(logging.DEBUG)\nlogging.basicConfig(format='%(levelname)s:%(message)s')\n\nclass Extractor:\n    def __init__(self):\n        self.browser = None\n        self.browser_times = []\n        self.processing_times = []\n        self.headless = True\n\n    async def start_browser(self, headless):\n        if self.browser is None or headless != self.headless:\n            await self.close_browser()\n            log.info(f\"Starting headless browser (headless={self.headless})...\")\n            self.headless = headless\n            self.browser = await pyppeteer.launch(headless=self.headless, args=[\"--no-sandbox\"])\n            logging.getLogger(\"pyppeteer\").setLevel(logging.WARNING)\n            log.info(\"Browser started.\")\n\n    async def close_browser(self):\n        if self.browser is not None:\n            if HAS_PYPPETEER:\n                await self.browser.close()\n            self.browser = None\n            log.info(\"\\nBrowser closed.\")\n\n    async def get_raw_content(self, url, request_strategy=\"requests\", headless=True, delay=0):\n        \"\"\"\n        Runs a HTTP(S) request to the url and returns the page's source code.\n\n        For the vast majority of pages, a simple GET request via the requests library is sufficient (request_strategy=\"requests\").\n\n        However, some website require additional interaction, e.g. JS or scrolling,\n        either due to overzealous DDOS protections like Cloudflare or due to dynamic loading strategies (Medium blogs are one example).\n        For these pages, Selenium can be used.\n        \"\"\"\n\n        start = time.time()\n        if request_strategy == \"requests\":\n            r = requests.get(url)\n            if not r.status_code == 200:\n                logging.warning(f\"Could not access {url}. HTTP status {r.status_code}\")\n                raise LookupError(\"HTTP non-200 response\")\n            raw_content = r.text\n\n        elif request_strategy == \"chrome\" and HAS_PYPPETEER:\n            await self.start_browser(headless)\n            page = await self.browser.newPage()\n            await asyncio.sleep(delay)\n            response = await page.goto(url)\n            if response.headers[\"status\"] != \"200\":\n                logging.error(f\"Status code: {response.headers['status']}\")\n                logging.error(f\"Full headers: {response.headers}\")\n                raise LookupError\n            raw_content = await page.content()\n            page = await page.close()\n\n        else:\n            logging.warning(f\"No viable implementation found for request strategy '{request_strategy}'\")\n            raw_content = \"\"\n            raise LookupError\n\n        end = time.time()\n        self.browser_times.append(end-start)\n        return raw_content\n\n    def normalize_space_in_string(self, s):\n        try:\n            return lxml.html.fromstring(s).xpath(\"normalize-space(.)\") if (len(s) and ord(s[0]) != 65279) else \"\"\n        except lxml.etree.ParserError:\n            return \"\"\n\n    def extract_items_by_xpath(self, item_els, fields):\n        item_strs_in_tree = []\n\n        if self.debug and True:\n            print(\"Item:\", etree.tostring(item_els[0], pretty_print=True, method=\"html\").decode('unicode_escape'))\n        if False:\n            print(\"Whole page:\", etree.tostring(tree, pretty_print=True, method=\"html\").decode('unicode_escape'))\n\n        for item_el in item_els:\n            item_str = []\n            for field in fields:\n                if \"extractor\" in field:\n                    result = field[\"extractor\"](item_el.value)\n                else:\n                    result = item_el.xpath(field['xpath'])\n                    if result and len(result):\n                        if isinstance(result, list):\n                            result = [self.normalize_space_in_string(str(a).strip()) for a in result if a and len(a.strip())]\n                            result = \", \".join(result)\n                        elif isinstance(result, str):\n                            result = self.normalize_space_in_string(str(result).strip())\n\n                if result:\n                    item_str.append(f\"{field['key']}: {result}\")\n            item_strs_in_tree.append(\"\\n\".join(item_str))\n\n        return item_strs_in_tree\n\n    def extract_items_by_jsonpath(self, item_els, fields):\n        item_strs_in_tree = []\n\n        for item_el in item_els:\n            item_str = []\n            for field in fields:\n                if \"extractor\" in field:\n                    result = field[\"extractor\"](item_el.value)\n                else:\n                    result = jsonpath_ng.parse(field[\"jsonpath\"]).find(item_el.value)\n                    if result and len(result):\n                        if isinstance(result, list):\n                            result = [(r.value or \"\") for r in result]\n                            result = \", \".join(result)\n\n                if result:\n                    item_str.append(f\"{field['key']}: {result}\")\n            item_strs_in_tree.append(\"\\n\".join(item_str))\n\n        return item_strs_in_tree\n\n\n    def run(self, spec, debug=False, outfile=\"/tmp/scraping_results\"):\n        all_item_strs = []\n        self.debug = debug\n\n        logging.info(f\"Attempting to scrape {spec['name']} ...\")\n\n        if not isinstance(spec[\"url_funcs\"], list):\n            spec[\"url_funcs\"] = [spec[\"url_funcs\"]]\n        if not isinstance(spec[\"url_funcs_args\"], list):\n            spec[\"url_funcs_args\"] = [spec[\"url_funcs_args\"]]\n\n        event_loop = asyncio.get_event_loop()\n        try:\n            for url_func, url_func_args in zip(spec[\"url_funcs\"], spec[\"url_funcs_args\"]):\n                i = -1\n                previous_result = None\n                next_url_gen = iter(url_func_args) if hasattr(url_func_args, '__iter__') else url_func_args\n                while True:\n                    i += 1\n                    if hasattr(url_func_args, '__iter__'):\n                        args = next(next_url_gen, None)\n                    else:\n                        args = url_func_args(previous_result)\n\n                    if args is None:  # this isn't the same as [None]\n                        break\n\n                    if self.debug and i > 1:\n                        break\n\n                    url = url_func(*args)\n\n                    if url is None or len(url) == 0:\n                        break\n\n                    print(f\"[{i}] Items already found: {len(all_item_strs)} (current url: {url})\", end=\"\\r\", flush=True)\n\n                    try:\n                        start = time.time()\n\n                        raw_content = event_loop.run_until_complete(self.get_raw_content(url, spec.get(\"request_strategy\", \"requests\"),\n                                                                                                   spec.get(\"headless\", True),\n                                                                                                   spec.get(\"delay\", 0)))\n\n                        if \"preprocessor\" in spec:\n                            raw_content = spec[\"preprocessor\"](raw_content)\n\n                        previous_result = raw_content\n\n                        item_strs = []\n                        if spec.get(\"content_type\", \"html\") == \"html\":\n                            tree = lxml.html.fromstring(raw_content)\n                            tree.make_links_absolute(url)\n                            item_els = tree.xpath(spec[\"items_path\"])\n\n                            if len(item_els) == 0:\n                                logging.info(f\"\\nNo {'more ' if len(all_item_strs) else ''}articles found, stopping.\")\n                                break\n\n                            item_strs = self.extract_items_by_xpath(item_els, spec[\"fields\"])\n\n                        elif spec.get(\"content_type\", \"json\") == \"json\":\n                            item_els = jsonpath_ng.parse(spec[\"items_path\"]).find(json.loads(raw_content))\n                            if len(item_els) == 0:\n                                logging.info(f\"\\nNo {'more ' if len(all_item_strs) else ''}articles found, stopping.\")\n                                break\n\n                            item_strs = self.extract_items_by_jsonpath(item_els, spec[\"fields\"])\n\n                        if item_strs is None:\n                            break\n\n                        len_before_new_items = len(all_item_strs)\n                        all_item_strs += item_strs\n\n                        # remove duplicates\n                        len_before_deduplication = len(all_item_strs)\n                        all_item_strs = list(set(all_item_strs))\n                        len_after_deduplication = len(all_item_strs)\n                        deduplication_difference = len_before_deduplication - len_after_deduplication\n\n                        if len_after_deduplication == len_before_new_items:\n                            logging.info(\"\\nNo more new items available, stopping.\")\n                            raise LookupError\n                        elif deduplication_difference > 0:\n                            logging.info(f\"\\nRemoved {deduplication_difference} duplicated item(s)\")\n\n                        end = time.time()\n                        self.processing_times.append(end - start)\n\n                    except LookupError:\n                        log.info(\"\\nLookup Error\")\n                        break\n                    except AttributeError:\n                        traceback.print_exc()\n                        break\n                    except ValueError:\n                        traceback.print_exc()\n                        break\n                    except KeyboardInterrupt:\n                        log.info(\"\\nReceived exit, exiting\")\n                        break\n                        event_loop.close()\n                        sys.exit()\n                    except RuntimeError:\n                        break\n                        traceback.print_exc()\n                        event_loop.close()\n                        sys.exit()\n                    except pyppeteer.errors.BrowserError as e:\n                        log.warning(f\"\\nBrowser Error: {e}\")\n                        traceback.print_exc()\n                        break\n                    except Exception as e:\n                        log.info(f\"\\nError while trying to scrape {url}: {e}\")\n                        traceback.print_exc()\n                        continue\n\n            log.info(f\"\\nProcessing complete. Items found: {len(all_item_strs)}.\")\n            log.info(f\"\\nRequest time: {sum(self.browser_times)}. Processing time: {sum(self.processing_times)}\")\n\n            if self.debug:\n                print(SEPARATOR.join(all_item_strs[0:5]))\n            else:\n                with open(outfile, \"w\") as f:\n                    f.write(SEPARATOR.join(all_item_strs))\n\n        finally:\n            event_loop.run_until_complete(self.close_browser())",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Existing specifications:",
   "metadata": {
    "tags": [],
    "cell_id": "00006-a82d3f0e-be76-4810-a1b9-0330de44a2b9",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-p"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-dcb2b235-f28c-4424-8943-fec9fdc19043",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "416ee692",
    "execution_start": 1627321372585,
    "execution_millis": 45,
    "is_code_hidden": false,
    "deepnote_cell_type": "code"
   },
   "source": "\nimport itertools\nfrom cssselect import GenericTranslator, SelectorError\ndef css(css_selector):\n    try:\n        return GenericTranslator().css_to_xpath(css_selector)\n    except SelectorError:\n        print('Invalid selector.')\n\nfrom datetime import datetime\n\ncset = {\n    \"name\": \"CSET\",\n    \"url_funcs\": (lambda page_index: f\"https://cset.georgetown.edu/publications/?fwp_paged={page_index}\"),\n    \"url_funcs_args\": [([i] for i in itertools.count(1))],\n    \"items_path\": css(\".teaser\"),\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": \".//h4[1]/a/text()\"\n    }, {\n        \"key\": \"Abstract\",\n        \"xpath\": \".//p[1]/text()\"\n    }, {\n        \"key\": \"Authors\",\n        \"xpath\": \".//div[@class='teaser__authors']/span/text()\"\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": \".//div[@class='teaser__top']/span[last()]/text()\"\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": \".//h4[1]//a[1]/@href\"\n    }]\n}\n\nbrookings = {\n    \"name\": \"Brookings\",\n    \"url_funcs\": [(lambda page_index: f\"https://www.brookings.edu/project/artificial-intelligence-and-emerging-technology-initiative/page/{page_index}/?type=research\"),\n                 (lambda page_index: f\"https://www.brookings.edu/topic/technology-innovation/page/{page_index}/?type=research\")],\n    \"url_funcs_args\": [([i] for i in itertools.count(1)),\n                      ([i] for i in itertools.count(1))],\n    \"items_path\": css(\".article-info\"),\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": \".//h4/a[1]/text()\"\n    }, {\n        \"key\": \"Authors\",\n        \"xpath\": \".//div[@class='authors']//span/text()\",\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": \".//time/text()\"\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": \".//h4/a[1]/@href\"\n    }]\n}\n\ncnas = {\n    \"name\": \"CNAS\",\n    \"url_funcs\": [(lambda page_index: f\"https://www.cnas.org/reports/p{page_index}\")],\n    \"url_funcs_args\": [([i] for i in itertools.count(1))],\n    \"items_path\": css(\".entry-listing\") + \"[1]/li\",\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": css(\".fz16\") + \"[1]/text()\",\n    }, {\n        \"key\": \"Authors\",\n        \"xpath\": css(\".fz15\") + \"[1]/text()[2]\"\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": \".//ul[1]/li[last()]/text()\"\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": css(\".fz16\") + \"[1]/@href\"\n    }]\n}\n\ncdi = {\n    \"name\": \"Center for Data Innovation\",\n    \"url_funcs\": [(lambda page_index: f\"https://datainnovation.org/category/publications/reports/page/{page_index}\")],\n    \"url_funcs_args\": [([i] for i in itertools.count(1))],\n    \"items_path\": css(\".header-list-style\"),\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": css(\".grid-title\") + \"[1]/a/text()\",\n    }, {\n        \"key\": \"Authors\",\n        \"xpath\": css(\".grid-post-box-meta\") + \"[1]/a/text()\"\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": css(\".published\") + \"[1]/text()\"\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": css(\".grid-title\") + \"[1]/a/@href\"\n    }]\n}\n\nitif = {\n    \"name\": \"Information Technology & Innovation Foundation\",\n    \"url_funcs\": [(lambda page_index: f\"https://itif.org/publications/reports-briefings?page={page_index}\")],\n    \"url_funcs_args\": [([i] for i in itertools.count(0))],\n    \"items_path\": css(\".views-row\"),\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": css(\".views-field-title\") + \"[1]//a/text()\",\n    }, {\n        \"key\": \"Summary\",\n        \"xpath\": css(\".views-field-body\") + \"[1]//\" + css(\".field-content\") + \"[1]/text()\"\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": css(\".views-field-field-date\") + \"[1]//\" + css(\".date-display-single\") + \"[1]/text()\"\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": css(\".views-field-title\") + \"[1]//a/@href\",\n    }]\n}\n\nrand = {\n    \"name\": \"RAND\",\n    \"url_funcs\": [(lambda page_index: f\"https://www.rand.org/topics/science-and-technology.html?page={page_index}\")],\n    \"url_funcs_args\": [([i] for i in itertools.count(1))],\n    \"items_path\": css(\"ul.teasers.list.organic\") + \"/li\",\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": css(\".title\") + \"[1]/a/text()\",\n    }, {\n        \"key\": \"Summary\",\n        \"xpath\": css(\"p\") + \"[1]/text()\",\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": css(\".date\") + \"/text()\"\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": css(\".title\") + \"[1]/a/@href\",\n    }]\n}\n\nrand = {\n    \"name\": \"RAND\",\n    \"url_funcs\": [(lambda page_index: f\"https://www.rand.org/topics/science-and-technology.html?page={page_index}\")],\n    \"url_funcs_args\": [([i] for i in itertools.count(1))],\n    \"items_path\": css(\"ul.teasers.list.organic\") + \"/li\",\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": css(\".title\") + \"[1]/a/text()\",\n    }, {\n        \"key\": \"Summary\",\n        \"xpath\": css(\"p\") + \"[1]/text()\",\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": css(\".date\") + \"/text()\"\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": css(\".title\") + \"[1]/a/@href\",\n    }]\n}\n\n\nbelfer = {\n    \"name\": \"Belfer Center\",\n    \"url_funcs\": [(lambda page_index: f\"https://www.belfercenter.org/research/publication-type/reports-papers?f%5B0%5D=topic%3AScience%20%26%20Technology&page={page_index}\")],\n    \"url_funcs_args\": [([i] for i in itertools.count(0))],\n    \"items_path\": css(\".teaser-body\"),\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": css(\".title\") + \"[1]/a/span/text()\",\n    }, {\n        \"key\": \"Summary\",\n        \"xpath\": css(\".field--name-field-summary\") + \"[1]/p/text()\",\n    }, {\n        \"key\": \"Authors\",\n        \"xpath\": css(\".author\") + \"/text()\",\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": css(\".pub-date\") + \"/text()\"\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": css(\".title\") + \"[1]/a/@href\",\n    }]\n}\n\n\ncsis = {\n    \"name\": \"Center for Strategic and International Studies\",\n    \"url_funcs\": [(lambda page_index:\n                   f\"https://www.csis.org/analysis?&type=publication&field_publication_type%5B1%5D=781&field_categories_field_topics%5B2%5D=822&page={page_index}\"\n                   )],\n    \"url_funcs_args\": [([i] for i in itertools.count(0))],\n    \"items_path\": css(\".ds-right\"),\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": css(\".teaser__title\") + \"[1]/a/text()\",\n    }, {\n        \"key\": \"Summary\",\n        \"xpath\": css(\".teaser__text\") + \"[1]/text()\",\n    }, {\n        \"key\": \"Authors\",\n        \"xpath\": css(\".teaser__expert\") + \"/a/text()\",\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": css(\".date-display-single\") + \"/text()\"\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": css(\".teaser__title\") + \"[1]/a/@href\",\n    }]\n}\n\n\nmitre = {\n    \"name\": \"MITRE\",\n    \"url_funcs\": [(lambda page_index:\n                    f\"https://www.mitre.org/publication-keywords/artificial-intelligence?page={page_index}\"\n                   ),\n                  (lambda page_index:\n                    f\"https://www.mitre.org/publication-keywords/computer-security?page={page_index}\"\n                   ),\n                  ],\n    \"url_funcs_args\": [([i] for i in itertools.count(0)), ([i] for i in itertools.count(0))],\n    \"items_path\": css(\".list-main.list-item\"),\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": css(\".title\") + \"[1]/text()\",\n    }, {\n        \"key\": \"Summary\",\n        \"xpath\": css(\".teaser\") + \"[1]/text()\",\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": css(\".date-display-single\") + \"/text()\"\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": \"a[1]/@href\",\n    }]\n}\n\n\ncsba = {\n    \"name\": \"Center for Strategic and Budgetary Assessments\",\n    \"url_funcs\": [(lambda page_index:\n                    f\"https://csbaonline.org/research/publications/P{page_index*6}?categories%5B%5D=132&categories%5B%5D=131\"\n                   ),\n                  ],\n    \"url_funcs_args\": [([i] for i in itertools.count(0))],\n    \"items_path\": css(\".research-publications-articles .article\"),\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": css(\".article-title\") + \"[1]/a/text()\",\n    }, {\n        \"key\": \"Summary\",\n        \"xpath\": \".//p/text()\",\n    }, {\n        \"key\": \"Authors\",\n        \"xpath\": css(\".article-meta\") + \"/a/text()\", #\"*[self::a or self::span[not(@class='sep')]]/text()\",\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": \".//time/text()\"\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": css(\".article-title\") + \"[1]/a/@href\",\n    }]\n}\n\n\nwilson = {\n    \"name\": \"Wilson Center\",\n    \"url_funcs\": [(lambda page_index:\n                   f\"https://www.wilsoncenter.org/insight-analysis?_page={page_index}&keywords=&_limit=10&programs=116,400,526&types=publication\"\n                   ),],\n    \"url_funcs_args\": [([i] for i in itertools.count(1))],\n    \"items_path\": css(\".teaser\"),\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": css(\".teaser-title-text\") + \"[1]/text()\",\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": \"./@href\",\n    }]\n}\n\nac = {\n    \"name\": \"Atlantic Council\",\n    \"url_funcs\": [(lambda page_index:\n                   f\"https://www.atlanticcouncil.org/insights-impact/research/?ac-page-collection-block_5d7284e0aa316={page_index}\"\n                   ),],\n    \"url_funcs_args\": [([i] for i in itertools.count(1))],\n    \"items_path\": css(\".gta-post-embed--content\"),\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": \"h4/a/text()\",\n    }, {\n        \"key\": \"Summary\",\n        \"xpath\": css(\".gta-post-embed--excerpt\") + \"/text()\",\n    }, {\n        \"key\": \"Authors\",\n        \"xpath\": css(\".gta-post-site-banner--tax--expert\") + \"/text()\",\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": css(\".gta-post-embed--heading\") + \"/text()\",\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": \"h4/a/@href\",\n    }]\n}\n\nnewamerica = {\n    \"name\": \"New America\",\n    \"url_funcs\": [(lambda previous_result:\n                        \"https://www.newamerica.org/api/post/?page_size=1000&content_type=report\" if previous_result is None else json.loads(previous_result)[\"next\"])],\n    \"url_funcs_args\": [lambda previous_result: [previous_result]],\n    \"items_path\": \"results.[*]\",\n    \"content_type\": \"json\",\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"jsonpath\": \"title\"\n    }, {\n        \"key\": \"Excerpt\",\n        \"jsonpath\": \"story_excerpt\"\n    }, {\n        \"key\": \"Authors\",\n        \"extractor\": lambda i: \", \".join([(a.get(\"first_name\", \"\") + \" \" + a.get(\"last_name\", \"\")) for a in (i.get(\"authors\", []) or [])]),\n    }, {\n        \"key\": \"Date\",\n        \"jsonpath\": \"date\"\n    }, {\n        \"key\": \"URL\",\n        \"extractor\": lambda i: f\"https://newamerica.org{i['url']}\"\n    }],\n}\n\n\nafsd = {\n    \"name\": \"Alliance for Securing Democracy\",\n    \"url_funcs\": [(lambda page_index:\n                   f\"https://securingdemocracy.gmfus.org/category/policy-paper/page/{page_index}\"\n                   ),],\n    \"url_funcs_args\": [([i] for i in itertools.count(1))],\n    \"items_path\": \".//article\",\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": css(\".entry-title\") + \"/a/text()\",\n    }, {\n        \"key\": \"Summary\",\n        \"xpath\": css(\".fusion-post-content-container\") + \"/p[1]/text()\",\n    }, {\n        \"key\": \"Authors\",\n        \"xpath\": css(\".vcard .fn\") + \"[1]/a/text()\",\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": css(\".asd-meta__date\") + \"/text()\",\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": css(\".entry-title\") + \"/a/@href\"\n    }]\n}\n\n\nrstreet = {\n    \"name\": \"R Street\",\n    \"url_funcs\": [(lambda page_index:\n                   f\"https://www.rstreet.org/publications/?_category=research-commentary&_issue=technology-innovation&_paged={page_index}\"\n                   ),],\n    \"url_funcs_args\": [([i] for i in itertools.count(1))],\n    \"request_strategy\": \"chrome\",\n    \"items_path\": css(\".w-full.pt-4\"),\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": css(\"a.mb-4\") + \"/text()\",\n    }, {\n        \"key\": \"Summary\",\n        \"xpath\": css(\".pt-4\") + \"/p[1]/text()\",\n    }, {\n        \"key\": \"Authors\",\n        \"xpath\": css(\".tracking-wider.text-sm\") + \"/text()\",\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": \".//time/text()\"\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": css(\"a.mb-4\") + \"/@href\",\n    }]\n}\n\n\nheritage = {\n    \"name\": \"Heritage Foundation\",\n    \"url_funcs\": [(lambda page_index:\n                    f\"https://www.heritage.org/technology?taxonomy_term_tid=143&f[0]=content_type%3Areport&page={page_index}\"\n                   ),\n                  (lambda page_index:\n                    f\"https://www.heritage.org/cybersecurity?taxonomy_term_tid=143&f[0]=content_type%3Areport&page={page_index}\"\n                   ),],\n    \"url_funcs_args\": [([i] for i in itertools.count(0)), ([i] for i in itertools.count(0))],\n    \"items_path\": css(\".result-card\"),\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": css(\".result-card__title\") + \"/text()\",\n    }, {\n        \"key\": \"Authors\",\n        \"xpath\": css(\".result-card__link\") + \"/span/text()\"\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": css(\".result-card__date\") + \"/span/text()\"\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": css(\".result-card__title\") + \"/@href\",\n    }]\n}\n\n\n\npeterson = {\n    \"name\": \"Peterson Institute for International Economics\",\n    \"url_funcs\": [(lambda page_index:\n                   f\"https://www.piie.com/research/publications/policy-briefs?page={page_index}\"\n                   ),],\n    \"url_funcs_args\": [([i] for i in itertools.count(0))],\n    \"items_path\": css(\".node--publication\"),\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": \".//h3/a/text()\",\n    }, {\n        \"key\": \"Authors\",\n        \"xpath\": css(\".field--contributor\") + \"/p/a/text()\"\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": css(\".date-display-single\") + \"/text()\"\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": \".//h3/a/@href\"\n    }]\n}\n\n\n# -----------------  NEW ADDITIONS TO THE LIST -----------------------\n\n\nfsi = {\n    \"name\": \"Stanford Freeman Spogli Institute, Internet Observatory\",\n    \"url_funcs\": [(lambda page_index: f\"https://cyber.fsi.stanford.edu/io/publications\")],\n    \"url_funcs_args\": [([i] for i in itertools.count(0))],\n    \"items_path\": css(\".views-row\"),\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": css(\".views-field-title\") + \"[1]//a/text()\",\n    }, {\n        \"key\": \"Authors\",\n        \"xpath\": css(\".views-field-field-related-authors\") + \"[1]//li/text()\",\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": css(\".views-field-title\") + \"[1]//a/@href\",\n    }]\n}\n\n\ndfrlab = {\n    \"name\": \"Digital Forensic Research Lab\",\n    \"url_funcs\": [\n        (lambda page_index, cur_timestamp: f\"https://medium.com/_/api/collections/df0d49d8c59b/topics/f6dfe506de26?to={cur_timestamp}&page={page_index}\"),  # technology\n        (lambda page_index, cur_timestamp: f\"https://medium.com/_/api/collections/df0d49d8c59b/topics/f361c4ce391a?to={cur_timestamp}&page={page_index}\"),  # democracy\n        (lambda page_index, cur_timestamp: f\"https://medium.com/_/api/collections/df0d49d8c59b/topics/5046526b459a?to={cur_timestamp}&page={page_index}\"),  # security\n        (lambda page_index, cur_timestamp: f\"https://medium.com/_/api/collections/df0d49d8c59b/topics/8893b0f7d055?to={cur_timestamp}&page={page_index}\"),  # contributors\n    ],\n    \"url_funcs_args\": [\n        ([i, int(datetime.now().timestamp() * 1000)] for i in itertools.count(1)),\n        ([i, int(datetime.now().timestamp() * 1000)] for i in itertools.count(1)),\n        ([i, int(datetime.now().timestamp() * 1000)] for i in itertools.count(1)),\n        ([i, int(datetime.now().timestamp() * 1000)] for i in itertools.count(1)),\n    ],\n    \"preprocessor\": lambda r: r[16:], # cut off first 16 letters of non-json gibberish\n    \"items_path\": \"payload.references.Post.*\",\n    \"content_type\": \"json\",\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"jsonpath\": \"title\"\n    }, {\n        \"key\": \"Summary\",\n        \"jsonpath\": \"content.subtitle\"\n    }, {\n        \"key\": \"Date\",\n        \"extractor\": lambda i: datetime.fromtimestamp(float(i['firstPublishedAt']) / 1000).strftime(\"%d %B, %Y\")\n    }, {\n        \"key\": \"URL\",\n        \"extractor\": lambda i: f\"https://medium.com/dfrlab/{i['uniqueSlug']}\"\n    }],\n}\n\naspi = {\n    \"name\": \"Australian Strategic Policy Institute\",\n    \"url_funcs\": [(lambda page_index: f\"https://www.aspi.org.au/search?sort_by=field_publication_date_common&page={page_index}\")],\n    \"url_funcs_args\": [([i] for i in itertools.count(0))],\n    \"request_strategy\": \"chrome\",\n    \"headless\": False,\n    \"delay\": 30,  # 30 second delay between pages, to avoid triggering the rate limiter\n    \"items_path\": \".//div[@role='article']\",\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": \".//h4/a/span/text()\"\n    }, {\n        \"key\": \"Authors\",\n        \"xpath\": css(\".group-right\") + \"[1]/p[last()]//a/text()\",\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": css(\".field--name-field-publication-date-common\") + \"[1]/text()\",\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": \".//h4/a/@href\"\n    }]\n}\n\nstopfake = {\n    \"name\": \"StopFake.org\",\n    \"url_funcs\": [(lambda page_index: f\"https://www.stopfake.org/en/category/research/page/{page_index}\")],\n    \"url_funcs_args\": [([i] for i in itertools.count(1))],\n    \"items_path\": css(\".item-details\"),\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": \".//h3/a/text()\"\n    }, {\n        \"key\": \"Excerpt\",\n        \"xpath\": css(\".td-excerpt\") + \"[1]/text()\",\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": \".//time/text()\"\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": \".//h3/a/@href\"\n    }]\n}\n\n\nstratcom = {\n    \"name\": \"NATO Strategic Communications Centre of Excellence\",\n    \"url_funcs\": [(lambda page_index: f\"https://stratcomcoe.org/publications?page={page_index}\")],\n    \"url_funcs_args\": [([i] for i in itertools.count(1))],\n    \"items_path\": css(\".item-content\"),\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": \".//h2/text()\"\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": css(\".date\") + \"[1]/text()\"\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": \"a/@href\"\n    }]\n}\n\n\nlowyinstitute = {\n    \"name\": \"Lowy Institute\",\n    \"url_funcs\": [(lambda page_index: f\"https://www.lowyinstitute.org/all/publications/page/{page_index}\")],\n    \"url_funcs_args\": [([i] for i in itertools.count(0))],\n    \"items_path\": css(\".node-content\"),\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": \".//h2/a/text()\"\n    }, {\n        \"key\": \"Summary\",\n        \"xpath\": css(\".summary_content\") + \"[1]/text()\"\n    }, {\n        \"key\": \"Authors\",\n        \"xpath\": css(\".submitted\") + \"[1]/a/text()\"\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": css(\".date\") + \"[1]/text()\"\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": \".//h2/a/@href\"\n    }]\n}\n\n\nfireeye = {\n    \"name\": \"FireEye Threat Intelligence Reports\",\n    \"url_funcs\": [(lambda _: f\"https://www.fireeye.com/current-threats/threat-intelligence-reports.html\")],\n    \"url_funcs_args\": [[[None]]],\n    \"items_path\": css(\".c01_item\"),\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": \".//a/text()\"\n    }, {\n        \"key\": \"Summary\",\n        \"xpath\": \".//span/text()\"\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": \".//a/@href\"\n    }]\n}\n\ngraphika = {\n    \"name\": \"Graphika Social Network Reports\",\n    \"url_funcs\": [(lambda _: f\"https://graphika.com/reports\")],\n    \"url_funcs_args\": [[[None]]],\n    \"items_path\": css(\".report-item\"),\n    \"fields\": [{\n        \"key\": \"Title\",\n        \"xpath\": \".//div//a/h2/text()\"\n    }, {\n        \"key\": \"Summary\",\n        \"xpath\": \".//div/p[@class='']/text()\"\n    }, {\n        \"key\": \"Date\",\n        \"xpath\": \".//small/text()\"\n    }, {\n        \"key\": \"URL\",\n        \"xpath\": \".//div//a/h2/parent::a/@href\"\n    }]\n}\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00009-cf9177a9-5ba0-4b7d-a349-859a0223942f",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "30ae5dc1",
    "execution_start": 1627321372637,
    "execution_millis": 829697,
    "deepnote_cell_type": "code"
   },
   "source": "\nsource_specs = [cset,\n                brookings,\n                cnas,\n                cdi,\n                itif,\n                rand,\n                belfer,\n                csis,\n                mitre,\n                csba,\n                wilson,\n                ac,\n                newamerica,\n                afsd,\n                rstreet,  # warning -- requires headless chrome\n                heritage,\n                peterson,\n                fsi,\n                dfrlab,\n                #aspi,  # exclude for now, due to cloudflare issues\n                stopfake,\n                stratcom,\n                lowyinstitute,\n                fireeye,\n                graphika]\n\ne = Extractor()\nfor s in source_specs:\n    e.run(s, debug=False, outfile=(\"./\" + s[\"name\"] + \".scrape.txt\"))\n",
   "outputs": [
    {
     "name": "stdout",
     "text": "",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "INFO:\nLookup Error\nINFO:\nProcessing complete. Items found: 227.\nINFO:\nRequest time: 8.654510021209717. Processing time: 8.752454042434692\nWARNING:Could not access https://www.brookings.edu/project/artificial-intelligence-and-emerging-technology-initiative/page/10/?type=research. HTTP status 404\nINFO:\nLookup Error\nWARNING:Could not access https://www.brookings.edu/topic/technology-innovation/page/54/?type=research. HTTP status 404\nINFO:\nLookup Error\nINFO:\nProcessing complete. Items found: 526.\nINFO:\nRequest time: 37.826335430145264. Processing time: 39.02401375770569\nINFO:\nLookup Error\nINFO:\nProcessing complete. Items found: 539.\nINFO:\nRequest time: 50.2677845954895. Processing time: 51.764790058135986\nWARNING:Could not access https://datainnovation.org/category/publications/reports/page/3. HTTP status 404\nINFO:\nLookup Error\nINFO:\nProcessing complete. Items found: 36.\nINFO:\nRequest time: 51.392231702804565. Processing time: 52.94012475013733\nINFO:\nProcessing complete. Items found: 652.\nINFO:\nRequest time: 98.71362638473511. Processing time: 100.4991204738617\nINFO:\nLookup Error\nINFO:\nProcessing complete. Items found: 1132.\nINFO:\nRequest time: 219.91992831230164. Processing time: 220.94064593315125\nINFO:\nProcessing complete. Items found: 348.\nINFO:\nRequest time: 287.39141392707825. Processing time: 288.9628851413727\nINFO:\nProcessing complete. Items found: 341.\nINFO:\nRequest time: 310.0901565551758. Processing time: 312.79345059394836\nINFO:\nProcessing complete. Items found: 133.\nINFO:\nRequest time: 321.44622683525085. Processing time: 323.93959856033325\nINFO:\nProcessing complete. Items found: 214.\nINFO:\nRequest time: 417.2854561805725. Processing time: 418.2484176158905\nINFO:\nProcessing complete. Items found: 139.\nINFO:\nRequest time: 437.083459854126. Processing time: 437.56574273109436\nINFO:\nLookup Error\nINFO:\nProcessing complete. Items found: 1082.\nINFO:\nRequest time: 504.0150909423828. Processing time: 504.9626636505127\nINFO:\nProcessing complete. Items found: 1327.\nINFO:\nRequest time: 614.8855936527252. Processing time: 642.1646506786346\nWARNING:Could not access https://securingdemocracy.gmfus.org/category/policy-paper/page/3. HTTP status 404\nINFO:\nLookup Error\nINFO:\nProcessing complete. Items found: 72.\nINFO:\nRequest time: 616.9173607826233. Processing time: 644.2712156772614\nINFO:Starting headless browser (headless=True)...\nWARNING:\nBrowser Error: Browser closed unexpectedly:\n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_78/311569555.py\", line 200, in run\n    spec.get(\"delay\", 0)))\n  File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/nest_asyncio.py\", line 70, in run_until_complete\n    return f.result()\n  File \"/usr/local/lib/python3.7/asyncio/futures.py\", line 181, in result\n    raise self._exception\n  File \"/usr/local/lib/python3.7/asyncio/tasks.py\", line 249, in __step\n    result = coro.send(None)\n  File \"/tmp/ipykernel_78/311569555.py\", line 82, in get_raw_content\n    await self.start_browser(headless)\n  File \"/tmp/ipykernel_78/311569555.py\", line 51, in start_browser\n    self.browser = await pyppeteer.launch(headless=self.headless, args=[\"--no-sandbox\"])\n  File \"/root/venv/lib/python3.7/site-packages/pyppeteer/launcher.py\", line 306, in launch\n    return await Launcher(options, **kwargs).launch()\n  File \"/root/venv/lib/python3.7/site-packages/pyppeteer/launcher.py\", line 167, in launch\n    self.browserWSEndpoint = get_ws_endpoint(self.url)\n  File \"/root/venv/lib/python3.7/site-packages/pyppeteer/launcher.py\", line 226, in get_ws_endpoint\n    raise BrowserError('Browser closed unexpectedly:\\n')\npyppeteer.errors.BrowserError: Browser closed unexpectedly:\n\nINFO:\nProcessing complete. Items found: 0.\nINFO:\nRequest time: 616.9173607826233. Processing time: 644.2712156772614\nINFO:\nProcessing complete. Items found: 72.\nINFO:\nRequest time: 635.0490181446075. Processing time: 661.3276581764221\nINFO:\nProcessing complete. Items found: 406.\nINFO:\nRequest time: 661.5800998210907. Processing time: 687.9520044326782\nINFO:\nLookup Error\nINFO:\nProcessing complete. Items found: 30.\nINFO:\nRequest time: 664.8090124130249. Processing time: 688.014972448349\nINFO:\nLookup Error\nINFO:\nProcessing complete. Items found: 89.\nINFO:\nRequest time: 668.2896194458008. Processing time: 692.2423062324524\nWARNING:Could not access https://www.stopfake.org/en/category/research/page/16. HTTP status 404\nINFO:\nLookup Error\nINFO:\nProcessing complete. Items found: 146.\nINFO:\nRequest time: 703.9622321128845. Processing time: 728.0690784454346\nINFO:\nProcessing complete. Items found: 195.\nINFO:\nRequest time: 711.5236701965332. Processing time: 735.6500389575958\nINFO:\nProcessing complete. Items found: 1419.\nINFO:\nRequest time: 751.7312605381012. Processing time: 776.1501798629761\nINFO:\nProcessing complete. Items found: 42.\nINFO:\nRequest time: 752.2275495529175. Processing time: 776.6681580543518\nINFO:\nProcessing complete. Items found: 41.\nINFO:\nRequest time: 752.5406010150909. Processing time: 777.0029809474945\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00003-fa34a82e-ecec-49c1-b6f6-f6da62925f19",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d87bcc35",
    "execution_start": 1627322202337,
    "execution_millis": 96,
    "deepnote_cell_type": "code"
   },
   "source": "from glob import glob\nimport os\n\n\ndef concat_files(file_list, outfilename):\n    with open(outfilename, \"w\") as outfile:\n        for filename in sorted(file_list):\n\n            print(\"joining\", filename)\n            with open(filename, \"r\") as infile:\n                outfile.write(infile.read())\n                outfile.write(SEPARATOR)\n\nconcat_files(glob('./*.scrape.txt'), \"./merged_scrape.txt\")\n#concat_files([(ss[\"name\"] + \".scrape.txt\") for ss in [fsi, dfrlab, stopfake, stratcom, lowyinstitute, fireeye, graphika]], \"./cybersecurity.scrape.txt\")\n",
   "outputs": [
    {
     "name": "stdout",
     "text": "joining ./Alliance for Securing Democracy.scrape.txt\njoining ./Atlantic Council.scrape.txt\njoining ./Belfer Center.scrape.txt\njoining ./Brookings.scrape.txt\njoining ./CNAS.scrape.txt\njoining ./CSET.scrape.txt\njoining ./Center for Data Innovation.scrape.txt\njoining ./Center for Strategic and Budgetary Assessments.scrape.txt\njoining ./Center for Strategic and International Studies.scrape.txt\njoining ./Digital Forensic Research Lab.scrape.txt\njoining ./FireEye Threat Intelligence Reports.scrape.txt\njoining ./Graphika Social Network Reports.scrape.txt\njoining ./Heritage Foundation.scrape.txt\njoining ./Information Technology & Innovation Foundation.scrape.txt\njoining ./Lowy Institute.scrape.txt\njoining ./MITRE.scrape.txt\njoining ./NATO Strategic Communications Centre of Excellence.scrape.txt\njoining ./New America.scrape.txt\njoining ./Peterson Institute for International Economics.scrape.txt\njoining ./R Street.scrape.txt\njoining ./RAND.scrape.txt\njoining ./Stanford Freeman Spogli Institute, Internet Observatory.scrape.txt\njoining ./StopFake.org.scrape.txt\njoining ./Wilson Center.scrape.txt\njoining ./cybersecurity.scrape.txt\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# WORK IN PROGRESS BELOW",
   "metadata": {
    "tags": [],
    "cell_id": "00010-eabdef8f-23ce-40d2-b6a8-d86396f58a45",
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00010-46a027df-8bf6-4ba6-8af3-6cdf966e58e0",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "89ec3011",
    "execution_start": 1627322202476,
    "execution_millis": 13,
    "deepnote_cell_type": "code"
   },
   "source": "from pyppeteer.launcher import Launcher\n' '.join(Launcher().cmd)",
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 6,
     "data": {
      "text/plain": "'/root/.local/share/pyppeteer/local-chromium/588429/chrome-linux/chrome --disable-background-networking --disable-background-timer-throttling --disable-breakpad --disable-browser-side-navigation --disable-client-side-phishing-detection --disable-default-apps --disable-dev-shm-usage --disable-extensions --disable-features=site-per-process --disable-hang-monitor --disable-popup-blocking --disable-prompt-on-repost --disable-sync --disable-translate --metrics-recording-only --no-first-run --safebrowsing-disable-auto-update --enable-automation --password-store=basic --use-mock-keychain --headless --hide-scrollbars --mute-audio about:blank --remote-debugging-port=35859 --user-data-dir=/root/.local/share/pyppeteer/.dev_profile/tmpsus9muup'"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00011-c77e46e1-69be-41a4-877b-828c9c79e7f9",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b08a744",
    "execution_start": 1627322202505,
    "execution_millis": 15,
    "deepnote_cell_type": "code"
   },
   "source": "#!/usr/bin/env python3\n\n\nfrom pprint import pprint\nfrom lxml import etree\nfrom lxml.etree import XML, ElementTree\nimport lxml.html\nfrom cssselect import GenericTranslator, SelectorError\nfrom itertools import combinations\ndef css(css_selector):\n    try:\n        return GenericTranslator().css_to_xpath(css_selector)\n    except SelectorError:\n        print('Invalid selector.')\n\nimport re\nfrom collections import defaultdict\n\ntree = lxml.html.fromstring(\"\"\"<html>\n  <div class=\"header\"><h1>Welcome</h1></div>\n  <div/>\n  <div class=\"article\">\n    <h2><a>John Doe</a></h2>\n    <p>Title: <span class=\"title small\">The great article</span><span class=\"annotation\">(out of print)</span></p>\n    <div />\n  </div>\n  <div class=\"article\">\n    <h2><a>Jane Smith</a></h2>\n    <p class=\"annotation\">Guest contributor</p>\n    <p>Title: <span>A small book</span>(out of stock)</p>\n    <div />\n  </div>\n</html>\"\"\")\nroottree = tree.getroottree()\n\ndef parse_template(template):\n    return [l.split(\": \") for l in template.split(\"\\n\")]\n\ntemplates = [\"\"\"Author: John Doe\nTitle: The great article\"\"\",\n\"\"\"Author: Jane Smith\nTitle: (out of stock)\"\"\"]\n\npotential_selectors = {}  # keys are element's memory addresses id(element), values are lists of possible selectors\ndef get_potential_selectors(e, allow_nonunique_classes=False):\n    \"\"\"\n    Returns (tag, [<a list of strategies>]) for the element, where each strategy can be one of the following:\n      * a set() of class names, some combination of which may correctly identify the element and its relevant siblings\n      * an integer, representing a fixed index (to differentiate this element from its siblings)\n      * nothing, if the element has no siblings\n    \"\"\"\n    if id(e) in potential_selectors:\n        return potential_selectors[id(e)]\n\n    # if there are multiple siblings\n    siblings = []\n    for s in e.itersiblings(preceding=False):\n        if s.tag == e.tag:\n            siblings.append(s)\n    for s in e.itersiblings(preceding=True):\n        if s.tag == e.tag:\n            siblings.append(s)\n\n    selectors = []\n    if siblings:\n        # filter by same tag\n        # does this one have a class that distinguishes it from the others?\n        e_classes = set(e.attrib.get(\"class\", \"\").split())\n        some_siblings_classes = set()\n\n        for s in siblings:\n            s_classes = s.attrib.get(\"class\", \"\").split()\n            some_siblings_classes = some_siblings_classes.union(*s_classes)\n\n        unique_identifying_classes = e_classes.difference(some_siblings_classes)\n        if unique_identifying_classes:\n            selectors.append(unique_identifying_classes)\n\n        if allow_nonunique_classes:\n            all_siblings_classes = set()\n            for s in siblings:\n                s_classes = s.attrib.get(\"class\", \"\").split()\n                all_siblings_classes = all_siblings_classes.intersection(set(s_classes))\n            nonunique_identifying_classes = e_classes.difference(unique_identifying_classes).difference(some_siblings_classes)\n\n            if nonunique_identifying_classes:\n                selectors.append(nonunique_identifying_classes)\n\n        parent_tree = etree.ElementTree(e.getparent())\n        path_full = parent_tree.getpath(e)\n        index_tag = path_full[-len(path_full.split(\"[\")[-1]):-1]\n        selectors.append(int(index_tag))\n\n    result = (e.tag, selectors or [\"\"])  # when no particular strategies have been identified, just use the string.\n    potential_selectors[id(e)] = result\n    return result\n\ndef product(*sequences):\n    '''Breadth First Search Cartesian Product'''\n    # sequences = tuple(tuple(seq) for seqin sequences)\n\n    def partitions(n, k):\n        for c in combinations(range(n+k-1), k-1):\n            yield (b-a-1 for a, b in zip((-1,)+c, c+(n+k-1,)))\n\n    max_position = [len(i)-1 for i in sequences]\n    for i in range(sum(max_position)):\n        for positions in partitions(i, len(sequences)):\n            try:\n                yield tuple(map(lambda seq, pos: seq[pos], sequences, positions))\n            except IndexError:\n                continue\n    yield tuple(map(lambda seq, pos: seq[pos], sequences, max_position))\n\ndef get_xpath_part(selector):\n    (tag, strategy) = selector\n    if isinstance(strategy, set): # classes\n        return tag + \"[@class and (\" + \" or \".join([f\"contains(concat(' ', normalize-space(@class), ' '), ' {c} ')\" for c in list(strategy)[:1]]) + \")]\"\n    elif isinstance(strategy, int): # index\n        return tag + \"[\" + str(strategy) + \"]\"\n    elif strategy == \"\": # no strategies, just use the tag itself\n        return tag\n\ndef compile_xpath(selectors):\n    return \"./\" + \"/\".join(get_xpath_part(s) for s in selectors[::-1])\n\ndef expand_selector_list(selector_list):\n    tags, strategy_lists = zip(*selector_list)\n    selector_strategies_generator = product(*strategy_lists)\n    for selector_strategy_combo in selector_strategies_generator:\n        yield compile_xpath(list(zip(tags, selector_strategy_combo)))\n\ndef check_has_child(parent_node, selector_list):\n    \"\"\"\n    Check if, for the given parent node, any of the many combinations of selectors for each of their children\n    actually match an element.\n    \"\"\"\n    return any(parent_node.xpath(selector) for selector in expand_selector_list(selector_list))\n\ndef find_likely_parent_item(template):\n    parents_lists = []\n    tls = parse_template(template)\n    for e in tree.iter():\n        for eti, et in enumerate(e.xpath(\"text()\")):\n            for (key, value) in tls:\n                if et == value:\n                    parents_lists.append([e] + list(e.iterancestors()))\n\n    # now, we want to find the first element that overlaps in all lists.\n    # all lists, of course, start with /html. So we go backwards and find the first one that diverges.\n    i = -1\n    while True:\n        if len(set(parents[i] for parents in parents_lists)) == 1:\n            i -= 1\n        else:\n            # first divergence at i\n            break\n\n    i += 1 # this is the index (some negative number) of the first candidate for being the article div or similar\n\n    # build up all of the possible strategies of constructing xpaths for each found element\n    potential_selector_lists = []\n    for parents in parents_lists:\n        potential_selector_list = []\n        for j, pe in enumerate(parents):\n            neg_index = j - len(parents)\n            may_be_article = True #neg_index >= i\n            potential_selector_list.append(get_potential_selectors(pe, allow_nonunique_classes=may_be_article))\n        potential_selector_lists.append(potential_selector_list)\n\n    # go up the tree, and see at which level we come across the most siblings for which (at least some of) the\n    # target-finding paths match, i.e. siblings which also have the target elements, i.e. siblings that are also article divs.\n    parent_scores = defaultdict(int)\n    for j in range(i, 0):  # starting with the first potential *article* element ...\n        p = parents[j]\n        parent_siblings = list(p.itersiblings(preceding=True)) + list(p.itersiblings(preceding=False))\n        parent_siblings_are_articles = [False] * len(parent_siblings)\n        for psi, ps in enumerate(parent_siblings):\n            # ... go through all of its siblings, and for each of them,\n            for potential_selector_list in potential_selector_lists:\n                # ... check if we can find each one of the targets' equivalents underneath this sibling\n\n                if check_has_child(ps, potential_selector_list[:j]):\n                    parent_siblings_are_articles[psi] = True\n                    break\n\n        parent_scores[j] = sum(parent_siblings_are_articles)\n\n    parent_item_index = max(parent_scores, key=parent_scores.get)\n    return parents_lists[0][parent_item_index]  # the most likely article div\n\nprint(find_likely_parent_item(templates[0]))\n",
   "outputs": [
    {
     "name": "stdout",
     "text": "<Element div at 0x7fd854d17e90>\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00012-738ebbb0-c96d-43c1-9f50-e96b87bf7f86",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b623e53d",
    "execution_start": 1627322202515,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# automagic scraping: the great adventure (below -- work in progress)",
   "metadata": {
    "tags": [],
    "cell_id": "00014-a6b70d95-f9a3-45a5-9b57-49216280c0de",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00014-a1ba18ae-9044-4d94-b10c-bcf9b396e9f5",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cdbf9c51",
    "execution_start": 1627322202587,
    "execution_millis": 52,
    "deepnote_cell_type": "code"
   },
   "source": "#!/usr/bin/env python3\n\nfrom pprint import pprint\nfrom lxml import etree\nfrom lxml.etree import XML, ElementTree\nimport lxml.html\nfrom cssselect import GenericTranslator, SelectorError\nimport itertools\nfrom functools import lru_cache\nimport copy\nimport numpy as np\n\ndef css(css_selector):\n    try:\n        return GenericTranslator().css_to_xpath(css_selector)\n    except SelectorError:\n        print('Invalid selector.')\n\nimport re\nfrom collections import defaultdict, namedtuple\n\nsample = \"\"\"<html>\n  <div class=\"header\">\n    <h1>Welcome</h1>\n  </div>\n  <div class=\"feature\">\n    <h2><span class=\"bold\">\n  </div>\n  <div class=\"article\">\n    <h2><a>John Doe</a></h2>\n    <h2><a>John Doe</a></h2>\n    <h2><a>John Doe</a></h2>\n    <h2><a>John Doe</a></h2>\n    <h2>John Doe</h2>\n    <p>Title: <span class=\"title small\">The great article</span><span class=\"annotation\">(out of print)</span></p>\n    <div><span>John</span> and <span>Doe</span> and <p>John</p> <p>Doe</p></div>\n  </div>\n  <div class=\"article\">\n    <h2><a>Jane Smith</a></h2>\n    <p class=\"annotation\">Guest contributor</p>\n    <p>Title: <span>A small book</span>(out of stock)</p>\n    <div />\n    <span>John</span>\n  </div>\n</html>\"\"\"\n\n#with open(\"./tests/websites/fsi.html\") as f:\n#    sample = f.read()\n\n\ntemplates = [\"\"\"Author: John Doe\nTitle: The great article\"\"\",\n\"\"\"Author: Jane Smith\nTitle: A small book\"\"\"]\n\n#templates = [\"\"\"Title: Contours and Controversies of Parler\n#Authors: David Thiel, Renee DiResta, Shelby Grossman, Elena Cryst\"\"\",\n#\"\"\"Title: DeZurdaTeam: A Twitter network promotes pro-Cuba hashtags (TAKEDOWN)\n#Authors: Elena Cryst, Shelby Perkins\"\"\"]\n\n\ndef template_prob_match(prob1, prob2):\n    total = 0\n    for l1, l2 in zip(prob1, prob2):\n        total += l1 * l2\n    return total\n\n\ntree = lxml.html.fromstring(sample)\n\nclass ContentElement():\n    def __init__(self, selector, parent, content, n_fields, n_templates):\n        self.parent = parent\n        self.selector = selector\n        self.content = content\n        self.n_fields = n_fields\n        self.n_templates = n_templates\n\n        self.template_field_prob = defaultdict(lambda: [0] * n_templates)  # [template][field] = 0.0\n        self.template_other_prob = 0.0\n        super().__init__()\n\n    def getparent(self):\n        return self.parent\n\n    def iterancestors(self):\n        return itertools.chain(iter([self.parent]), self.parent.iterancestors())\n\n    def iterchildren(self):\n        return iter([])\n\n    def get_xored_template_field_prob(self, ):\n        \"\"\"\n        Return a version of self.template_field_prob that is collapsed along the fields,\n        showing the probability that this will take on exactly one field (XOR) for each template.\n        \"\"\"\n\n        def xor_values(probs):\n            negated_m = np.tile(np.array(probs)[None, :], [len(probs), 1])\n            m = (1 - negated_m) + 2 * np.eye(len(probs)) * (negated_m - .5)\n            return np.sum(np.prod(m, 1))\n\n        all_template_prob = [[] for ti in range(self.n_templates)]  # for each template index, a list of all probs any field ever had.\n        for field, template_prob in self.template_field_prob.items():\n            for ti, prob in enumerate(template_prob):\n                all_template_prob[ti].append(prob)\n\n        xored_values = [0] * self.n_templates\n        for ti, probs in enumerate(all_template_prob):\n            xored_values[ti] = xor_values(probs)\n\n        return xored_values\n\n\n    def update_template_other_prob(self):\n        total = 0\n        for field_probs in self.template_field_prob.values():\n            for prob in field_probs:\n                total += prob\n\n        self.template_other_prob = 1 - total\n\n    def __hash__(self):\n        return hash((self.parent, self.selector))\n\n    def __repr__(self):\n        return f\"<{self.parent.tag} {self.selector}={self.content}>\"\n\n\n# we first want to find all of the instances of the values.\nclass PathInductor():\n    def __init__(self, tree, templates):\n        self.tree = tree\n        self.templates = templates\n        self.parsed_templates = [self.parse_template(t) for t in templates]\n        self.fields = set().union(*[list(tp.keys()) for tp in self.parsed_templates])\n        self.n_fields = len(self.fields)\n        self.n_templates = len(self.templates)\n\n        self.field_prob = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))  # by (field, level)  # TODO competition between items, such that a field knows to belong somewhere\n\n        self.itemness_prob = defaultdict(float) # by (element) -- this is computed by the model\n        self.item_field_level_prob = defaultdict(float)  # by (field, level) -- this is after checking for presence of children\n\n        self.template_prob = defaultdict(lambda: [1] * (self.n_templates)) # by (element)\n        self.template_other_prob = defaultdict(lambda: float) # by (element)\n        self.template_field_prob = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n        self.field_selectors = defaultdict(lambda: defaultdict(float)) # by (field, level)\n        self.item_selectors = []\n\n        self.content_elements_by_parent = defaultdict(set)\n        self.content_elements_by_content = defaultdict(set)\n        self.template_fields_by_value = defaultdict(set)\n\n    def parse_template(self, template):\n        return {l.split(\": \", 1)[0]: l.split(\": \", 1)[1] for l in template.split(\"\\n\")}\n\n    @lru_cache(None)  # elements (e) are hashable!\n    def get_element_selector_options(self, e, allow_nonunique_classes=False):\n        if isinstance(e, ContentElement):\n            return [(None, e.selector)]\n\n        # if there are multiple siblings\n        siblings = list(e.itersiblings(e.tag, preceding=False)) + list(e.itersiblings(e.tag, preceding=True))\n\n        selectors = []\n        if siblings:\n            # filter by same tag\n            # does this one have a class that distinguishes it from the others?\n            e_classes = set(e.attrib.get(\"class\", \"\").split())\n            some_siblings_classes = set()\n\n            for s in siblings:\n                s_classes = s.attrib.get(\"class\", \"\").split()\n                some_siblings_classes = some_siblings_classes.union(*s_classes)\n\n            unique_identifying_classes = frozenset(e_classes.difference(some_siblings_classes))\n            if unique_identifying_classes:\n                selectors.append(unique_identifying_classes) #{\"strategy\": \"classes\", \"tag\": e.tag, \"classes\": unique_identifying_classes})\n\n            if allow_nonunique_classes:\n                all_siblings_classes = set()\n                for s in siblings:\n                    s_classes = s.attrib.get(\"class\", \"\").split()\n                    all_siblings_classes = all_siblings_classes.intersection(set(s_classes))\n                nonunique_identifying_classes = frozenset(e_classes.difference(unique_identifying_classes).difference(some_siblings_classes))\n\n                if nonunique_identifying_classes:\n                    selectors.append(nonunique_identifying_classes)\n\n            parent_tree = etree.ElementTree(e.getparent())\n            path_full = parent_tree.getpath(e)\n            index_tag = path_full[-len(path_full.split(\"[\")[-1]):-1]\n            selectors.append(int(index_tag))\n\n        result = [(e.tag, s) for s in (selectors or [\"\"])]  # when no particular strategies have been identified, just use the string.o\n        return result\n\n    def get_xpath_part(self, selector):\n        (tag, strategy) = selector\n        if tag is None:\n            return strategy\n        elif isinstance(strategy, set) or isinstance(strategy, frozenset): # classes\n            return tag + \"[@class and (\" + \" or \".join([f\"contains(concat(' ', normalize-space(@class), ' '), ' {c} ')\" for c in list(strategy)]) + \")]\"\n        elif isinstance(strategy, int): # index\n            return tag + \"[\" + str(strategy) + \"]\"\n        elif strategy == \"\": # no strategies, just use the tag itself\n            return tag\n\n    def compile_xpath(self, selectors, global_path=False):\n        return (\"//\" if global_path else \"./\") + \"/\".join(self.get_xpath_part(s) for s in selectors[::-1])\n\n    def find_candidate_content_elements(self, value):\n        candidates = []\n        for e in self.tree.iter():\n            for eti, et in enumerate(e.xpath(\"text()\")):\n                if et == value:\n                    if eti == 0:\n                        le = ContentElement(\"text()\", e, value, self.n_fields, self.n_templates)\n                    else:\n                        le = ContentElement(f\"text()[{eti + 1}]\", e, value, self.n_fields, self.n_templates)\n                    candidates.append(le)\n                    self.content_elements_by_content[value].add(le)\n                    self.content_elements_by_parent[e].add(le)\n\n            for eti, et in enumerate(e.xpath(\"@href\")):\n                if et == value:\n                    le = ContentElement(\"@href\", e, value, self.n_fields, self.n_templates)\n                    candidates.append(le)\n                    self.content_elements_by_content[value].add(le)\n                    self.content_elements_by_parent[e].add(le)\n\n        return candidates\n\n    def initialize_leaf_probs(self):\n        for ti, pt in enumerate(self.parsed_templates):\n            for field, value in pt.items():\n                candidate_content_elements = self.find_candidate_content_elements(value)\n                template_prob = 1. / len(candidate_content_elements)\n                for ce in candidate_content_elements:\n                    self.field_prob[field][ce][0] = 1.0\n                    ce.template_field_prob[field][ti] = template_prob\n                    self.template_fields_by_value[value].add((ti, field))\n\n        for ce in set.union(*self.content_elements_by_parent.values()):\n            ce.update_template_other_prob()\n\n    def _update_template_prob(self, element):\n        \"\"\"\n        The probability of a parent being e.g. template 1 is the probability that:\n        - all children are *either* template1 fields, or other, AND\n        - all children are *not* just other fields.\n\n        Furthermore, we assume that it can never happen that e.g. a template1 child and a template2 child are under the same parent.\n        Because those cases aren't captured in our calculation at all, the resulting probabilities won't add to 1, and we have to explicitly renormalize them.\n\n        In this case, we look through all children, and ask all children to report their template probs to us.\n\n        \"\"\"\n        template_prob = None\n        template_other_prob = 1\n        for child in itertools.chain(element.iterchildren(), self.content_elements_by_parent[element]):  # if this is a regular element with children\n            child_template_prob = None\n            if isinstance(child, lxml.etree.ElementBase):\n                child_template_prob = self._update_template_prob(child)\n                child_other_prob = self.template_other_prob[child]\n\n            elif isinstance(child, ContentElement):\n                child_template_prob = child.get_xored_template_field_prob()\n                child_other_prob = child.template_other_prob\n                print(\"child template prob\", child, child_template_prob)\n\n            if child_template_prob:\n                if template_prob is None:\n                    template_prob = [1] * self.n_templates\n                for ti, l in enumerate(child_template_prob):\n                    template_prob[ti] *= (l + child_other_prob)\n                template_other_prob *= child_other_prob\n\n        if template_prob is not None:\n            # subtract template_other_prob from everything\n            for ti, l in enumerate(template_prob):\n                template_prob[ti] -= template_other_prob\n\n            # then renormalize\n            total = sum(template_prob) + template_other_prob\n            for ti, l in enumerate(template_prob):\n                if total > 0:\n                    template_prob[ti] /= total\n\n            self.template_prob[element] = template_prob\n            self.template_other_prob[element] = (template_other_prob / total) if total > 0 else 0.0\n            return template_prob\n        else:\n            return False\n\n    def _update_field_prob(self, element, field):\n        element_field_prob = defaultdict(float)\n        for child in itertools.chain(element.iterchildren(), self.content_elements_by_parent[element]):\n            child_field_probs = None\n            match = 0\n            if child in self.field_prob[field]:\n                child_field_probs = self.field_prob[field][child]  # these are directly from below, e.g. [0]: 1\n                if isinstance(child, lxml.etree.ElementBase):\n                    match = sum(c * e for c, e in zip(self.template_prob[child], self.template_prob[element]))  # simple dot product\n                elif isinstance(child, ContentElement):\n                    match = sum(c * e for c, e in zip(child.template_field_prob[field], self.template_prob[element]))\n            else:\n                # or it doesn't, then we want to go one level below\n                child_field_probs = self._update_field_prob(child, field)\n                match = sum(c * e for c, e in zip(self.template_prob[child], self.template_prob[element]))  # simple dot product\n\n            if child_field_probs:\n                for level, level_prob in child_field_probs.items():\n                    element_field_prob[level + 1] += (level_prob * match)\n\n        if element_field_prob:\n            self.field_prob[field][element] = element_field_prob\n            return element_field_prob\n        else:\n            return False\n\n    def propagate_coverage_up(self):\n        self._update_template_prob(tree)\n        for field in self.fields:\n            self._update_field_prob(tree, field)\n\n        print(\"template prob\")\n        pprint(self.template_prob)\n        print(\"field prob\")\n        pprint(self.field_prob)\n\n    def distribute_belonging_down(self):\n        # first, we have the parent element's new field_probs, and also its template_probs. These together\n        # form a multiplicative table.\n        # first, we go down once and copy down all of the parent nodes' template_probs, unless they're all zero.\n\n        def set_parent_template_prob_recursively(element):\n            for child in element.iterchildren():\n                if isinstance(child, lxml.etree.ElementBase):\n                    if sum(self.template_prob[element]):\n                        self.template_prob[child] = self.template_prob[element][:] # deep copy just in case\n                    set_parent_template_prob_recursively(child)\n\n        set_parent_template_prob_recursively(tree)\n\n        # then, we go down once more and compute a new template_field_probs table for each leaf parent, to be normalized later.\n        parent_template_field_probs = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n        def set_parent_template_field_probs_recursively(element):\n            for child in element.iterchildren():\n                if isinstance(child, lxml.etree.ElementBase):\n                    for field, elementlevelprobs in self.field_prob.items():\n                        for element, levelprobs in elementlevelprobs.items():\n                            if 0 in levelprobs:\n                                # this could include fields that some selector thought were a good idea, but which don't actually match the example's value\n                                for ti, template_prob in enumerate(self.template_prob[element]):\n                                    parent_template_field_probs[element][field] = levelprobs[0] * np.array(self.template_prob[element])\n                    set_parent_template_field_probs_recursively(child)\n\n        set_parent_template_field_probs_recursively(tree)\n\n        # then, we go down once more and, for each one of the competing children, we collect all of the parents and normalize them\n        totals = defaultdict(lambda: defaultdict(lambda: np.array([0] * self.n_templates)))\n        for value, children in self.content_elements_by_content.items():\n            for child in children:\n                ptfb = parent_template_field_probs[child.getparent()]\n                for field, template_probs in ptfb.items():\n                    for ti, tprob in enumerate(template_probs):\n                        if (ti, field) in self.template_fields_by_value[child.content]:\n                            totals[value][field] += template_probs\n\n        # then, go through the children and normalize their respective parent's beliefs\n        child_totals = defaultdict(float)\n        for value, children in self.content_elements_by_content.items():\n            for child in children:\n                ptfb = parent_template_field_probs[child.getparent()]\n                for field, template_probs in ptfb.items():\n                    self.template_field_prob[child][field] = ptfb[field] / totals[value][field]\n                    child_totals[child] += np.sum(ptfb[field] / totals[value][field])\n                self.template_other_prob[child] = 1 - child_totals[child]\n\n\n\n    def update_item_model(self, full_items_only=False):\n        \"\"\"\n        Go through all of the elements for which we have field_prob\n        *or* through all which qualify as items, and count the occurence (or absence) of covered fields.\n\n        TODO if not full_items_only, consider missing fields too\n        \"\"\"\n        element_has_all_fields = defaultdict(lambda: False)\n        if full_items_only:\n            element_has_fields = defaultdict(set)\n            for field, elementlevelprobs in self.field_prob.items():\n                for element, levelprobs in elementlevelprobs.items():\n                    for level, prob in levelprobs.items():\n                        element_has_fields[element].add(field)\n                    for element, fields in element_has_fields.items():\n                        element_has_all_fields[element] = len(fields) == self.n_fields\n\n        field_level_combos_found = defaultdict(int)\n        levels_found_by_field = defaultdict(int)\n        for field, elementlevelprobs in self.field_prob.items():\n            for element, levelprobs in elementlevelprobs.items():\n                for level, prob in levelprobs.items():\n                    if (not full_items_only) or element_has_all_fields[element]:\n                        field_level_combos_found[field, level] += prob\n                        levels_found_by_field[field] += prob\n\n        # an item: for each field, has a certain distribution what the levels are:\n        for (field, level), combos_found in field_level_combos_found.items():\n            self.item_field_level_prob[field, level] = (combos_found / levels_found_by_field[field]) if levels_found_by_field[field] else 0.0\n\n        # when an element has a high probability of being a parent of multiple fields, its itemness goes up.\n        # however, we then infer the selectors for items, and next time, only the items with high selector prob become items.\n        # TODO also generate what kinds of items these things are\n        element_itemness_estimate = defaultdict(lambda: defaultdict(float))\n\n        for field, elementlevelprobs in self.field_prob.items():\n            for element, levelprobs in elementlevelprobs.items():\n                for level, prob in levelprobs.items():\n                    if (not full_items_only) or element_has_all_fields[element]:\n                        element_itemness_estimate[element][field, level] += prob * self.item_field_level_prob[field, level]\n\n        # clear out previous itemness_probs, then fill them in if the item_prob is large enough.\n        self.itemness_prob = defaultdict(float)\n        for element, fieldprobs in element_itemness_estimate.items():\n            element_item_prob = sum(fieldprobs.values()) / len(self.fields)  # itemprob is the average prob over the fields.\n            if element_item_prob > 0.01:\n                self.itemness_prob[element] = element_item_prob\n            else:\n                self.itemness_prob.pop(element, None)\n\n\n    def generate_field_selectors(self):\n        # now we want to update the probabilities of each selector going up\n        # for each field and level, we have a dictionary of selectors with their probabilities.\n        self.field_selectors = defaultdict(lambda: defaultdict(float)) # by (field, level)\n        for field in self.fields:\n            elements_to_update_selectors = set()  # set of (element, level)\n            for ce, levelprobs in self.field_prob[field].items():\n                if not 0 in levelprobs: # we are just looking for elements that occur at level 0, i.e. leaves\n                    continue\n                elements_to_update_selectors.add((ce, 0))\n                for level, ancestor in enumerate(ce.iterancestors(), 1):\n                    elements_to_update_selectors.add((ancestor, level))\n\n            for element, level in elements_to_update_selectors:\n                options = self.get_element_selector_options(element)\n                element_prob = self.field_prob[field][element][level]\n                for option in options:\n                    self.field_selectors[field, level][option] += element_prob\n\n        # prune selectors with low probabilities\n        for (field, level), selector_options in self.field_selectors.items():\n            prunable_options = [selector_option for selector_option, prob in selector_options.items() if prob < 0.01]\n            for po in prunable_options:\n                del selector_options[po]\n\n        # we have a selector for elements that are (field: URL, generation: 2).\n        # the correct one is /div[@class='details']/, but because the URL is just a span, there are a lot of\n        # grandparents of spans.\n        # TODO clean up and prune the list of selectors for each kind of element\n        # for each level, we want to look at the probabilities, and only keep the top 10 selectors\n        #print(\"field selectors are:\")\n        #pprint(dict(self.field_selectors))\n        # TODO especially if there are highly common intersections of classes, find the most likely set of classes\n\n\n    def generate_item_selectors(self):\n        # TODO choose items that have enough itemness, pruning some if necessary, and generate selectors for those.\n        itemness_by_tag = defaultdict(dict)\n        for element, itemness in self.itemness_prob.items():\n            itemness_by_tag[element.tag][element] = itemness\n\n        element_by_tag = defaultdict(list)\n        for tag, element_itemness_probs in itemness_by_tag.items():\n            # for all of the ones that have the same tag, see which ones have the same parent.\n            element_by_parent = defaultdict(list)\n            for element, itemness_prob in element_itemness_probs.items():\n                element_by_parent[element.getparent()].append((element, itemness_prob,))\n\n            # for each parent, compute the total score:\n            parent_scores = defaultdict(float)\n            for parent, elementprobs in element_by_parent.items():\n                for element, prob in elementprobs:\n                    parent_scores[parent] += prob\n            # get the parent with the highest score\n            best_parent = max(parent_scores, key=parent_scores.get)\n            element_by_tag[tag] = element_by_parent[best_parent]\n\n        tag_scores = defaultdict(float)\n        for tag, elementprobs in element_by_tag.items():\n            for element, prob in elementprobs:\n                tag_scores[tag] += prob\n        best_tag = max(tag_scores, key=tag_scores.get)\n        best_items = [t for (t, p) in element_by_tag[best_tag]]\n        # now generate some selector that captures all of the items but nothing else\n        best_item_selectors = [self.get_element_selector_options(t) for t in best_items]\n        # first, try merging the selectors' class sets\n        selector = (best_item_selectors[0][0][0], frozenset(set.intersection(*[set(s[0][1]) for s in best_item_selectors])))\n        item_field_level_coverage = defaultdict(set)\n        for item in best_items:\n            for field, elementprobs in self.field_prob.items():\n                for level, prob in elementprobs[item].items():\n                    if prob > 0.01:\n                        item_field_level_coverage[field].add(level)\n        self.item_selectors.append((selector, 1, item_field_level_coverage))\n\n\n    def select_and_check_item_candidates(self):\n\n        def recursively_detect_coverage(element, field, level):\n            if level > 2:\n                candidate_children = set()\n                for selector in self.field_selectors[field, level - 1].keys():\n                    for child in element.xpath(self.compile_xpath([selector])):\n                        candidate_children.add(child)\n                return any([recursively_detect_coverage(child, field, level - 1) for child in candidate_children])\n\n            elif level == 2:\n                # only one needs to match\n                leaf_selectors = self.field_selectors[field, 1].keys()  # the text() has been found in p's and in spans. could be both.\n                content_element_selectors = self.field_selectors[field, 0].keys()\n                for (leaf_selector, content_element_selector) in itertools.product(leaf_selectors, content_element_selectors):\n                    if len(element.xpath(self.compile_xpath([content_element_selector, leaf_selector]))):\n                        return True\n                return False\n\n        # TODO update the itemness probs based on the item selectors\n        coverage = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n        for (selector, prob, item_field_level_coverage) in self.item_selectors:\n            for el in tree.xpath(self.compile_xpath([selector], global_path=True)):\n                # now, for each one of these found items, we want to check what their itemness would be.\n                for field in self.fields:\n                    for level in item_field_level_coverage[field]:\n                        # we think it might be possible that the item contains (field @ level).\n                        # we check this by recursively checking for the presence of such an element.\n                        # these items, most likely, are not examples we have already discovered.\n                        # so when we discover particular items near the bottom of the chain,\n                        # we want to figure out the coverage, which will most likely just be \"1\",\n                        # and we will then normalize this by the end of it.\n                        field_level_coverage = recursively_detect_coverage(el, field, level)\n                        if field_level_coverage:\n                            coverage[field][el][level] = True\n\n        # TODO: don't update coverage for items that we already have a prob for\n        # TODO: normalize\n        # TODO check if close enough to item model, kick out if not\n        # TODO update item model accordingly.\n        print(\"coverage\", coverage)\n\n    # now we want to run the loop where we continuously update the probabilities\n    def approximate_probabilities(self):\n        self.initialize_leaf_probs()  # does the search and propagates the field_probs upwards through the tree\n        self.propagate_coverage_up()  # TODO this is clearly not correct at the moment; shouldn't be using logit addition but rather something smarter.\n        self.distribute_belonging_down()\n\n        # now let's look at what the items are like.\n        self.update_item_model(full_items_only=True)\n\n        # build the initial item model based on full-coverage items only.\n        self.generate_field_selectors()\n\n        self.generate_item_selectors()  # find item selectors for the top n items.\n        self.select_and_check_item_candidates()  # select a bunch of other candidates for items, and also their item types (i.e. likely level combos)\n        return\n\n\npi = PathInductor(tree, templates)\npi.approximate_probabilities()\n",
   "outputs": [
    {
     "name": "stdout",
     "text": "child template prob <a text()=John Doe> [0.20000000000000007, 0.0]\nchild template prob <a text()=John Doe> [0.20000000000000007, 0.0]\nchild template prob <a text()=John Doe> [0.20000000000000007, 0.0]\nchild template prob <a text()=John Doe> [0.20000000000000007, 0.0]\nchild template prob <h2 text()=John Doe> [0.20000000000000007, 0.0]\nchild template prob <span text()=The great article> [1.0, 0.0]\nchild template prob <a text()=Jane Smith> [0.0, 1.0]\nchild template prob <span text()=A small book> [0.0, 1.0]\ntemplate prob\ndefaultdict(<function PathInductor.__init__.<locals>.<lambda> at 0x7fd854fb38c0>,\n            {<a text()=John Doe>: [1, 1],\n             <a text()=John Doe>: [1, 1],\n             <a text()=John Doe>: [1, 1],\n             <a text()=John Doe>: [1, 1],\n             <span text()=A small book>: [1, 1],\n             <span text()=The great article>: [1, 1],\n             <a text()=Jane Smith>: [1, 1],\n             <h2 text()=John Doe>: [1, 1],\n             <Element p at 0x7fd854d17350>: [1, 1],\n             <Element span at 0x7fd854d174d0>: [1, 1],\n             <Element p at 0x7fd854d175f0>: [1, 1],\n             <Element p at 0x7fd854d17650>: [0.0, 1.0],\n             <Element h2 at 0x7fd854d17890>: [0.0, 1.0],\n             <Element p at 0x7fd854d179b0>: [1, 1],\n             <Element span at 0x7fd854d17ad0>: [1, 1],\n             <Element span at 0x7fd854d17d70>: [1, 1],\n             <Element div at 0x7fd854d17ef0>: [1, 1],\n             <Element html at 0x7fd854e60a10>: [0.0, 0.0],\n             <Element span at 0x7fd854edb350>: [1, 1],\n             <Element div at 0x7fd855205050>: [0.0, 1.0],\n             <Element h2 at 0x7fd8552051d0>: [0.19999999999999996, 0.0],\n             <Element div at 0x7fd855205290>: [1.0, 0.0],\n             <Element div at 0x7fd8552052f0>: [1, 1],\n             <Element h2 at 0x7fd855205350>: [1, 1],\n             <Element h2 at 0x7fd8552053b0>: [0.19999999999999996, 0.0],\n             <Element a at 0x7fd855205410>: [0.0, 1.0],\n             <Element h2 at 0x7fd855205590>: [0.19999999999999996, 0.0],\n             <Element a at 0x7fd8552055f0>: [0.19999999999999996, 0.0],\n             <Element span at 0x7fd855205770>: [1, 1],\n             <Element h2 at 0x7fd855205830>: [0.19999999999999996, 0.0],\n             <Element span at 0x7fd855205890>: [0.0, 1.0],\n             <Element a at 0x7fd8552058f0>: [0.19999999999999996, 0.0],\n             <Element h2 at 0x7fd855205a10>: [0.19999999999999996, 0.0],\n             <Element span at 0x7fd855205ad0>: [1.0, 0.0],\n             <Element body at 0x7fd855205b30>: [0.0, 0.0],\n             <Element h1 at 0x7fd855205b90>: [1, 1],\n             <Element div at 0x7fd855205c50>: [1, 1],\n             <Element a at 0x7fd855205cb0>: [0.19999999999999996, 0.0],\n             <Element p at 0x7fd855205d10>: [1.0, 0.0],\n             <Element a at 0x7fd855205e30>: [0.19999999999999996, 0.0],\n             <Element div at 0x7fd856424b30>: [1, 1]})\nfield prob\ndefaultdict(<function PathInductor.__init__.<locals>.<lambda> at 0x7fd855011950>,\n            {'Author': defaultdict(<function PathInductor.__init__.<locals>.<lambda>.<locals>.<lambda> at 0x7fd855010950>,\n                                   {<a text()=John Doe>: defaultdict(<class 'float'>,\n                                                                     {0: 1.0}),\n                                    <a text()=John Doe>: defaultdict(<class 'float'>,\n                                                                     {0: 1.0}),\n                                    <a text()=John Doe>: defaultdict(<class 'float'>,\n                                                                     {0: 1.0}),\n                                    <a text()=John Doe>: defaultdict(<class 'float'>,\n                                                                     {0: 1.0}),\n                                    <a text()=Jane Smith>: defaultdict(<class 'float'>,\n                                                                       {0: 1.0}),\n                                    <h2 text()=John Doe>: defaultdict(<class 'float'>,\n                                                                      {0: 1.0}),\n                                    <Element h2 at 0x7fd854d17890>: defaultdict(<class 'float'>,\n                                                                                {2: 1.0}),\n                                    <Element html at 0x7fd854e60a10>: defaultdict(<class 'float'>,\n                                                                                  {4: 0.0,\n                                                                                   5: 0.0}),\n                                    <Element div at 0x7fd855205050>: defaultdict(<class 'float'>,\n                                                                                 {3: 1.0}),\n                                    <Element h2 at 0x7fd8552051d0>: defaultdict(<class 'float'>,\n                                                                                {1: 0.039999999999999994}),\n                                    <Element div at 0x7fd855205290>: defaultdict(<class 'float'>,\n                                                                                 {2: 0.007999999999999997,\n                                                                                  3: 0.0012799999999999988}),\n                                    <Element h2 at 0x7fd8552053b0>: defaultdict(<class 'float'>,\n                                                                                {2: 0.001599999999999999}),\n                                    <Element a at 0x7fd855205410>: defaultdict(<class 'float'>,\n                                                                               {1: 1.0}),\n                                    <Element h2 at 0x7fd855205590>: defaultdict(<class 'float'>,\n                                                                                {2: 0.001599999999999999}),\n                                    <Element a at 0x7fd8552055f0>: defaultdict(<class 'float'>,\n                                                                               {1: 0.039999999999999994}),\n                                    <Element h2 at 0x7fd855205830>: defaultdict(<class 'float'>,\n                                                                                {2: 0.001599999999999999}),\n                                    <Element a at 0x7fd8552058f0>: defaultdict(<class 'float'>,\n                                                                               {1: 0.039999999999999994}),\n                                    <Element h2 at 0x7fd855205a10>: defaultdict(<class 'float'>,\n                                                                                {2: 0.001599999999999999}),\n                                    <Element body at 0x7fd855205b30>: defaultdict(<class 'float'>,\n                                                                                  {3: 0.0,\n                                                                                   4: 0.0}),\n                                    <Element a at 0x7fd855205cb0>: defaultdict(<class 'float'>,\n                                                                               {1: 0.039999999999999994}),\n                                    <Element a at 0x7fd855205e30>: defaultdict(<class 'float'>,\n                                                                               {1: 0.039999999999999994})}),\n             'Title': defaultdict(<function PathInductor.__init__.<locals>.<lambda>.<locals>.<lambda> at 0x7fd855010a70>,\n                                  {<span text()=A small book>: defaultdict(<class 'float'>,\n                                                                           {0: 1.0}),\n                                   <span text()=The great article>: defaultdict(<class 'float'>,\n                                                                                {0: 1.0}),\n                                   <Element p at 0x7fd854d17650>: defaultdict(<class 'float'>,\n                                                                              {2: 1.0}),\n                                   <Element html at 0x7fd854e60a10>: defaultdict(<class 'float'>,\n                                                                                 {5: 0.0}),\n                                   <Element div at 0x7fd855205050>: defaultdict(<class 'float'>,\n                                                                                {3: 1.0}),\n                                   <Element div at 0x7fd855205290>: defaultdict(<class 'float'>,\n                                                                                {3: 1.0}),\n                                   <Element span at 0x7fd855205890>: defaultdict(<class 'float'>,\n                                                                                 {1: 1.0}),\n                                   <Element span at 0x7fd855205ad0>: defaultdict(<class 'float'>,\n                                                                                 {1: 1.0}),\n                                   <Element body at 0x7fd855205b30>: defaultdict(<class 'float'>,\n                                                                                 {4: 0.0}),\n                                   <Element p at 0x7fd855205d10>: defaultdict(<class 'float'>,\n                                                                              {2: 1.0})})})\ncoverage defaultdict(<function PathInductor.select_and_check_item_candidates.<locals>.<lambda> at 0x7fd854ce2e60>, {'Author': defaultdict(<function PathInductor.select_and_check_item_candidates.<locals>.<lambda>.<locals>.<lambda> at 0x7fd854ce2dd0>, {<Element div at 0x7fd855205290>: defaultdict(<class 'float'>, {3: True}), <Element div at 0x7fd855205050>: defaultdict(<class 'float'>, {3: True})}), 'Title': defaultdict(<function PathInductor.select_and_check_item_candidates.<locals>.<lambda>.<locals>.<lambda> at 0x7fd854ce2ef0>, {<Element div at 0x7fd855205290>: defaultdict(<class 'float'>, {3: True}), <Element div at 0x7fd855205050>: defaultdict(<class 'float'>, {3: True})})})\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "f26d0218-e0e7-4547-992b-fca4e3972c1e",
  "deepnote_execution_queue": []
 }
}